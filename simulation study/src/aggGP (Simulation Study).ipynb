{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1wXMNry7Hkyp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-jwidyawati/.local/lib/python3.10/site-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.5.1 is installed, but it is not compatible with the installed jaxlib version 0.5.3, so it will not be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import NUTS, MCMC\n",
    "from numpyro.infer import SVI, Trace_ELBO, Predictive\n",
    "import numpyro.diagnostics\n",
    "\n",
    "from termcolor import colored\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import dill\n",
    "import pickle\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rVNPzYx8nf5m",
    "outputId": "3648c721-36be-47c5-95e4-e4153ccaf9e9"
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Use GPU 1 (index 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZReN7eiHoiS"
   },
   "source": [
    "#GP Kernel Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "u-WVCaHGHru2"
   },
   "outputs": [],
   "source": [
    "def dist_euclid(x, z):\n",
    "    \"\"\"\n",
    "    Computes Eucledian Distance Between Regions. This function is used by\n",
    "    exp_sq_kernel function (kernel function for gaussian processes)\n",
    "    \"\"\"\n",
    "    x = jnp.array(x) # (ngrid_pts, lat/lon) <- i.e (7304,2)\n",
    "    z = jnp.array(z) # (ngrid_pts, lat/lon) <- i.e (7304,2)\n",
    "    if len(x.shape)==1:\n",
    "        x = x.reshape(x.shape[0], 1) #(2618,) -> (7304,1)\n",
    "    if len(z.shape)==1:\n",
    "        z = x.reshape(x.shape[0], 1) #(2618,) -> (7304,1)\n",
    "    n_x, m = x.shape # 7304 , 2\n",
    "    n_z, m_z = z.shape # 7304 , 2\n",
    "    assert m == m_z\n",
    "    delta = jnp.zeros((n_x,n_z)) #(ngrid_pts,ngrid_pts) <- i.e (7304,7304)\n",
    "    for d in jnp.arange(m):\n",
    "        x_d = x[:,d] #(ngrid_pts-lat/lon,) <- (7304,)\n",
    "        z_d = z[:,d] #(ngrid_pts-lat/lon,) <- (7304,)\n",
    "        delta += (x_d[:,jnp.newaxis] - z_d)**2 # (7304,7304)\n",
    "\n",
    "    return jnp.sqrt(delta) #(7304,7304)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gekmJsOwHumV"
   },
   "outputs": [],
   "source": [
    "def exp_sq_kernel(x, z, var, length, noise, jitter=1.0e-4):\n",
    "    dist = dist_euclid(x, z) #(7304, 7304)\n",
    "    deltaXsq = jnp.power(dist/ length, 2.0)\n",
    "    k = var * jnp.exp(-0.5 * deltaXsq)\n",
    "    k += (noise + jitter) * jnp.eye(x.shape[0])\n",
    "    return k # (ngrid_pts, ngrid_pts) <- (7304,7304)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iOiO_1NHwNz"
   },
   "source": [
    "#Aggregation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xgPKoYAkHxrl"
   },
   "outputs": [],
   "source": [
    "def M_g(M, g):\n",
    "    '''\n",
    "    - $M$ is a matrix with binary entries $m_{ij},$ showing whether point $j$ is in polygon $i$\n",
    "    - $g$ is a vector of GP draws over grid\n",
    "    - $maltmul(M, g)$ gives a vector of sums over each polygon\n",
    "    '''\n",
    "    M = jnp.array(M)\n",
    "    g = jnp.array(g).T\n",
    "    return(jnp.matmul(M, g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qNASOC3HzmW"
   },
   "source": [
    "#Aggregated Prevalence Model - must edit this to include HDI, population density\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SQPuzilzH1ih"
   },
   "outputs": [],
   "source": [
    "def prev_model_gp_aggr(args):\n",
    "    \"\"\"Dengue prevalence model with a Gaussian Process\"\"\"\n",
    "\n",
    "    x = args[\"x\"]  # Spatial grid points: (num_grid_points, 2)\n",
    "    gp_kernel = args[\"gp_kernel\"]  # Gaussian Process kernel\n",
    "    noise = args[\"noise\"]\n",
    "    jitter = args[\"jitter\"]\n",
    "\n",
    "    pop_density_lo = args[\"pop_density_lo\"]  # (4,) one province (jkt) for 4 yrs' data\n",
    "    pop_density_hi = args[\"pop_density_hi\"]  # (24,)\n",
    "\n",
    "    #aggregate pop_density tgt\n",
    "    pop_density = jnp.concatenate([pop_density_lo,pop_density_hi], axis = 0)\n",
    "\n",
    "    #aggregate hdi tgt\n",
    "\n",
    "    hdi_lo = args[\"hdi_lo\"]  # (4,) 6 districts within jkt for 4 yrs' data\n",
    "    hdi_hi = args[\"hdi_hi\"]  # (24,)\n",
    "    hdi = jnp.concatenate([hdi_lo,hdi_hi], axis = 0)\n",
    "\n",
    "    M_lo = args[\"M_lo\"]  # (4, num_grid_points) aggregation matrix\n",
    "    M_hi = args[\"M_hi\"]  # (24, num_grid_points) aggregation matrix\n",
    "\n",
    "    total_cases_lo = args[\"total_cases_lo\"] #cos we wanna predict total cases district-wise, so only feed total cases for low res data\n",
    "\n",
    "    total_population_lo = args[\"total_population_lo\"]\n",
    "    total_population_hi = args[\"total_population_hi\"]\n",
    "\n",
    "    #aggregate total populations low and high\n",
    "    total_population = jnp.concatenate([total_population_lo,total_population_hi], axis = 0)\n",
    "\n",
    "    #add NaN values to total_cases to accommodate for unavailable total cases data for high resolution (that we want to predict)\n",
    "    total_cases = jnp.pad(total_cases_lo, (0, M_hi.shape[0]),constant_values = 0.0) #[3762.  484. ... , 0,0,0]\n",
    "    total_cases = jnp.where(total_cases == 0, jnp.nan, total_cases)# [3762.  484. ... , nan,nan,nan]\n",
    "    total_cases_mask = ~jnp.isnan(total_cases) # [True, True, ...., False, False, False]\n",
    "\n",
    "    # GP hyperparameters\n",
    "    kernel_length = numpyro.sample(\"kernel_length\", args[\"kernel_length\"])\n",
    "    kernel_var = numpyro.sample(\"kernel_var\", args[\"kernel_var\"])\n",
    "\n",
    "    # GP Kernel and Sample\n",
    "    k = gp_kernel(x, x, kernel_var, kernel_length, noise, jitter)\n",
    "    f = numpyro.sample(\"f\", dist.MultivariateNormal(loc=jnp.zeros(x.shape[0]), covariance_matrix=k))  # (num_grid_points,)\n",
    "\n",
    "    # Aggregate GP values to district level\n",
    "    gp_aggr_lo = numpyro.deterministic(\"gp_aggr_lo\", M_g(M_lo, f))  # (4,)\n",
    "    gp_aggr_hi = numpyro.deterministic(\"gp_aggr_hi\", M_g(M_hi, f))  # (24,)\n",
    "\n",
    "    # Now we need to aggregate both. This step is important since even though we only\n",
    "    # show the model the low resolution data, to produce high resolution data it\n",
    "    # needs th GP realizations for those regions\n",
    "    gp_aggr = numpyro.deterministic(\"gp_aggr\", jnp.concatenate([gp_aggr_lo,gp_aggr_hi])) #(28,)\n",
    "\n",
    "    # Fixed effects\n",
    "    b0 = numpyro.sample(\"b0\", dist.Normal(0, 1))  # Intercept\n",
    "    b_pop_density = numpyro.sample(\"b_pop_density\", dist.Normal(0, 1))  # Effect of population density\n",
    "    b_hdi = numpyro.sample(\"b_hdi\", dist.Normal(0, 1))  # Effect of HDI\n",
    "\n",
    "    #standardise pop_density and hdi before passing to the linear function\n",
    "    pop_density = (pop_density - jnp.mean(pop_density)) / jnp.std(pop_density)\n",
    "    hdi = (hdi - jnp.mean(hdi)) / jnp.std(hdi)\n",
    "\n",
    "    # Linear predictor\n",
    "    lp = b0 + gp_aggr + b_pop_density * pop_density + b_hdi * hdi  # (num_districts,)\n",
    "\n",
    "    # Prevalence probability\n",
    "    theta = numpyro.deterministic(\"theta\", jax.nn.sigmoid(lp) * 1e-2)  # (num_districts,)\n",
    "\n",
    "    # Binomial likelihood\n",
    "    with numpyro.handlers.mask(mask=total_cases_mask):\n",
    "        observed_cases = numpyro.sample(\n",
    "            \"observed_cases\",\n",
    "            dist.Binomial(total_count=total_population, probs=theta),\n",
    "            obs=total_cases)\n",
    "\n",
    "    return observed_cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbDDNakYIbYe"
   },
   "source": [
    "#Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3_l-ejoAIc2h"
   },
   "outputs": [],
   "source": [
    "# Lat/Lon Values of artificial grid\n",
    "x = np.load(\"../data/lat_lon_x_jkt.npy\")\n",
    "\n",
    "# combined regional data\n",
    "pol_pts_jkt_lo = np.load(\"../data/pol_pts_jkt_lo.npy\")\n",
    "pt_which_pol_jkt_lo = np.load(\"../data/pt_which_pol_jkt_lo.npy\")\n",
    "pol_pts_jkt_hi = np.load(\"../data/pol_pts_jkt_hi.npy\")\n",
    "pt_which_pol_jkt_hi = np.load(\"../data/pt_which_pol_jkt_hi.npy\")\n",
    "\n",
    "#combine the dataframes\n",
    "df_lo = gpd.read_file(\"../data/jkt_prov.shp\")\n",
    "df_hi = gpd.read_file(\"../data/jkt_dist.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cE5BfS4I5oZ"
   },
   "source": [
    "#Vars needed to be changed (change according to the agg prevalence model parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ivFcMASHI8W8"
   },
   "outputs": [],
   "source": [
    "M_lo = jnp.array(pol_pts_jkt_lo)\n",
    "M_hi = jnp.array(pol_pts_jkt_hi)\n",
    "pop_density_lo = jnp.array(df_lo[\"Pop_den\"])\n",
    "pop_density_hi = jnp.array(df_hi[\"Pop_den\"])\n",
    "hdi_lo = jnp.array(df_lo[\"HDI\"])\n",
    "hdi_hi = jnp.array(df_hi[\"HDI\"])\n",
    "cases_lo = jnp.array(df_lo[\"Cases\"])\n",
    "pop_lo = jnp.array(df_lo[\"Population\"])\n",
    "pop_hi = jnp.array(df_hi[\"Population\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j79GvFnGGlol",
    "outputId": "b4b37b17-a256-49f0-e2b5-dc453c9232f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1176)\n",
      "(24, 1176)\n",
      "(4,)\n",
      "(24,)\n",
      "(4,)\n",
      "(24,)\n",
      "(4,)\n",
      "(4,)\n",
      "(24,)\n",
      "(1176, 2)\n"
     ]
    }
   ],
   "source": [
    "#print the shape of all the vars above\n",
    "print(M_lo.shape)\n",
    "print(M_hi.shape)\n",
    "print(pop_density_lo.shape)\n",
    "print(pop_density_hi.shape)\n",
    "print(hdi_lo.shape)\n",
    "print(hdi_hi.shape)\n",
    "print(cases_lo.shape)\n",
    "print(pop_lo.shape)\n",
    "print(pop_hi.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWzo1haaJQwA"
   },
   "source": [
    "#Agg GP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2S8IwtvDJF-K"
   },
   "outputs": [],
   "source": [
    " args = {\n",
    "        \"x\" : jnp.array(x), # Lat/lon vals of grid points # Shape (num_districts, 2)\n",
    "        \"gp_kernel\" : exp_sq_kernel,\n",
    "        \"jitter\" : 1e-4,\n",
    "        \"noise\" : 1e-4,\n",
    "        \"M_lo\" : M_lo, # Aggregation matrix # Shape (num_districts, num_districts)\n",
    "        \"M_hi\" : M_hi, # Aggregation matrix # Shape (num_districts, num_districts)\n",
    "        # GP Kernel Hyperparams\n",
    "        \"kernel_length\" : dist.InverseGamma(3,3), #(,)\n",
    "        \"kernel_var\" : dist.HalfNormal(1e-5),\n",
    "        \"pop_density_lo\": pop_density_lo, # Shape (num_districts,)\n",
    "        \"pop_density_hi\": pop_density_hi, # Shape (num_districts,)\n",
    "        \"hdi_lo\": hdi_lo, # Shape (num_districts, 2)\n",
    "        \"hdi_hi\": hdi_hi, # Shape (num_districts, 2)\n",
    "        \"total_cases_lo\" : cases_lo,\n",
    "        \"total_population_lo\" : pop_lo,\n",
    "        \"total_population_hi\" : pop_hi,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doOpZ-ltJTlb"
   },
   "source": [
    "#Run MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0PVn1gP1JUhB"
   },
   "outputs": [],
   "source": [
    "# Base seed for reproducibility\n",
    "base_seed = 3  # Keep this fixed for full replicability\n",
    "\n",
    "\n",
    "# MCMC settings\n",
    "n_warm = 1000\n",
    "n_samples = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8TsnIhztMVBV"
   },
   "outputs": [],
   "source": [
    "# Directory for saving\n",
    "save_dir = \"../model_weights/aggGP\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWUOUxUeJWMC"
   },
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZWFS6twjv8qg",
    "outputId": "5468b9d5-cbad-46f0-c4bd-08dc6f3eaef7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Chain 4/4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 3000/3000 [19:36<00:00,  2.55it/s, 127 steps of size 3.69e-02. acc. prob=0.93]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Chain 4 to ../model_weights/aggGP_chain4_nsamples_2000_tt20min.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run MCMC with 1 chain at a time\n",
    "\n",
    "for chain_id in range(3, 4):\n",
    "    print(f\"Starting Chain {chain_id+1}/4...\")\n",
    "\n",
    "    # 🔹 Generate a unique but reproducible key for each chain\n",
    "    chain_key = jax.random.fold_in(jax.random.PRNGKey(base_seed), chain_id)\n",
    "\n",
    "    mcmc = MCMC(\n",
    "        NUTS(prev_model_gp_aggr),\n",
    "        num_warmup=n_warm,\n",
    "        num_samples=n_samples,\n",
    "        num_chains=1  # Run one chain at a time\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    try:\n",
    "        mcmc.run(chain_key, args)  # Use the fixed but different seed for each chain\n",
    "        end = time.time()\n",
    "        t_elapsed_min = round((end - start) / 60)\n",
    "\n",
    "        # 🔹 Save after each successful chain\n",
    "        f_path = f\"{save_dir}/aggGP_chain{chain_id+1}_nsamples_{n_samples}_tt{t_elapsed_min}min.pkl\"\n",
    "        with open(f_path, \"wb\") as file:\n",
    "            dill.dump(mcmc, file)\n",
    "\n",
    "        print(f\"Saved Chain {chain_id+1} to {f_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred in Chain {chain_id+1}: {e}\")\n",
    "        break  # Stop if an error occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No file found for chain 1\n",
      "Warning: No file found for chain 2\n",
      "Warning: No file found for chain 3\n",
      "Warning: No file found for chain 4\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     all_samples\u001b[38;5;241m.\u001b[39mappend(samples)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Concatenate all chains into a single dataset\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m combined_samples \u001b[38;5;241m=\u001b[39m {k: jnp\u001b[38;5;241m.\u001b[39mconcatenate([chain[k] \u001b[38;5;28;01mfor\u001b[39;00m chain \u001b[38;5;129;01min\u001b[39;00m all_samples], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[43mall_samples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()}\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Now, `combined_samples` contains all samples as if they were run together\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Directory where chains are saved\n",
    "n_samples = 2000  # Adjust based on your actual number of samples\n",
    "\n",
    "# Load all chains\n",
    "all_samples = []\n",
    "for chain_id in range(4):  # Assuming 4 chains were run\n",
    "    f_path = f\"{save_dir}/aggGP_chain{chain_id+1}_nsamples_{n_samples}_tt*.pkl\"\n",
    "    \n",
    "    # Find the actual file (handles varying runtimes)\n",
    "    matching_files = [f for f in os.listdir(save_dir) if f.startswith(f\"aggGP_chain{chain_id+1}_nsamples_{n_samples}_tt\")]\n",
    "    if not matching_files:\n",
    "        print(f\"Warning: No file found for chain {chain_id+1}\")\n",
    "        continue\n",
    "\n",
    "    # Load the most recent matching file\n",
    "    with open(os.path.join(save_dir, matching_files[-1]), \"rb\") as file:\n",
    "        mcmc = dill.load(file)\n",
    "    \n",
    "    # Extract and store samples\n",
    "    samples = mcmc.get_samples()\n",
    "    all_samples.append(samples)\n",
    "\n",
    "# Concatenate all chains into a single dataset\n",
    "combined_samples = {k: jnp.concatenate([chain[k] for chain in all_samples], axis=0) for k in all_samples[0].keys()}\n",
    "\n",
    "# Now, `combined_samples` contains all samples as if they were run together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ohvACXZ2l-HP"
   },
   "outputs": [],
   "source": [
    "samples = combined_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1Gj9ZQQmDr_",
    "outputId": "3879c32f-ec32-4c7b-eecb-7186e13b01e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.00156246, 0.00166599, 0.00197981, ..., 0.00054636, 0.00042465,\n",
       "        0.00047888],\n",
       "       [0.00154138, 0.0016863 , 0.00196741, ..., 0.00156476, 0.00173186,\n",
       "        0.00189528],\n",
       "       [0.0015445 , 0.00166163, 0.00202189, ..., 0.00046507, 0.00034778,\n",
       "        0.00039928],\n",
       "       ...,\n",
       "       [0.00157014, 0.00169491, 0.00201603, ..., 0.00062144, 0.00054654,\n",
       "        0.00061551],\n",
       "       [0.00146823, 0.00159232, 0.00192691, ..., 0.00059346, 0.0005004 ,\n",
       "        0.00056985],\n",
       "       [0.00150618, 0.00163585, 0.00193726, ..., 0.00077861, 0.00074849,\n",
       "        0.00083699]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view the theta\n",
    "samples[\"theta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZAE96PymKlP",
    "outputId": "f62ea8b2-bf71-4f8f-96eb-217b161b347b"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m n_lo \u001b[38;5;241m=\u001b[39m df_lo\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m n_hi \u001b[38;5;241m=\u001b[39m df_hi\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m ss \u001b[38;5;241m=\u001b[39m \u001b[43mnumpyro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiagnostics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m r \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(ss[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgp_aggr\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_eff\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage ESS for all aggGP effects : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(r)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpyro/diagnostics.py:270\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(samples, prob, group_by_chain)\u001b[0m\n\u001b[1;32m    268\u001b[0m median \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmedian(value_flat, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    269\u001b[0m hpd \u001b[38;5;241m=\u001b[39m hpdi(value_flat, prob\u001b[38;5;241m=\u001b[39mprob)\n\u001b[0;32m--> 270\u001b[0m n_eff \u001b[38;5;241m=\u001b[39m \u001b[43meffective_sample_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m r_hat \u001b[38;5;241m=\u001b[39m split_gelman_rubin(value)\n\u001b[1;32m    272\u001b[0m hpd_lower \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m50\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m prob))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpyro/diagnostics.py:176\u001b[0m, in \u001b[0;36meffective_sample_size\u001b[0;34m(x, bias)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03mComputes effective sample size of input ``x``, where the first dimension of\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m``x`` is chain dimension and the second dimension of ``x`` is draw dimension.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m:rtype: numpy.ndarray\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    175\u001b[0m x \u001b[38;5;241m=\u001b[39m device_get(x)\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# find autocovariance for each chain at lag k\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#assess the R hat, ESS etc.,\n",
    "n_lo = df_lo.shape[0]\n",
    "n_hi = df_hi.shape[0]\n",
    "\n",
    "ss = numpyro.diagnostics.summary(combined_samples)\n",
    "r = np.mean(ss[\"gp_aggr\"][\"n_eff\"])\n",
    "print(f\"Average ESS for all aggGP effects : {round(r)}\")\n",
    "\n",
    "ess_lo = np.mean(ss[\"gp_aggr\"][\"n_eff\"][0:n_lo])\n",
    "r_hat_lo = np.max(ss[\"gp_aggr\"][\"r_hat\"][0:n_lo])\n",
    "\n",
    "ess_hi = np.mean(ss[\"gp_aggr\"][\"n_eff\"][n_lo:n_lo + n_hi])\n",
    "r_hat_hi = np.max(ss[\"gp_aggr\"][\"r_hat\"][n_lo : n_lo + n_hi])\n",
    "\n",
    "print(f\"Average ESS for all aggGP-low effects : {round(ess_lo)}\")\n",
    "print(f\"Max r_hat for all aggGP-low : {round(r_hat_lo,2)}\")\n",
    "\n",
    "print(f\"Average ESS for all aggGP-high effects : {round(ess_hi)}\")\n",
    "print(f\"Max r_hat for all aggGP-high : {round(r_hat_hi,2)}\")\n",
    "\n",
    "print(f\"kernel_length R-hat : {round(ss['kernel_length']['r_hat'], 2)}\")\n",
    "print(f\"kernel_var R-hat : {round(ss['kernel_var']['r_hat'],2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IwWHLugpnWth",
    "outputId": "1fe5a450-161d-4ef1-b490-96532a5b8a6e"
   },
   "outputs": [],
   "source": [
    "# Convert summary stats to a DataFrame for better readability\n",
    "df_summary = pd.DataFrame.from_dict(ss, orient=\"index\")\n",
    "print(df_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "90XDSwaCnmnp",
    "outputId": "37dd671a-bf23-4b64-8d82-1e6be577bd4c"
   },
   "outputs": [],
   "source": [
    "# Convert manually combined samples into an ArviZ InferenceData object\n",
    "case_samples_az_gp = az.from_dict(posterior=combined_samples)\n",
    "\n",
    "# Plot trace plots for kernel_length and kernel_var\n",
    "az.plot_trace(case_samples_az_gp, var_names=[\"kernel_length\"])\n",
    "az.plot_trace(case_samples_az_gp, var_names=[\"kernel_var\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBWNi9gBrYnn"
   },
   "source": [
    "### Calculate and append observed and gp-estimated theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LkssrPezsA0o",
    "outputId": "ff4ef741-1cb7-43a9-d8a7-63925a4623df"
   },
   "outputs": [],
   "source": [
    "samples[\"theta\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3v7wGkjgqIyc"
   },
   "outputs": [],
   "source": [
    "theta_mean_gp = samples[\"theta\"].mean(axis = 0)\n",
    "bci_gp_25 = np.quantile(samples[\"theta\"],0.25,axis = 0)\n",
    "bci_gp_75 = np.quantile(samples[\"theta\"],0.75, axis = 0)\n",
    "\n",
    "df_lo[\"obs_prev\"] = df_lo[\"Cases\"] / df_lo[\"Population\"]\n",
    "df_hi[\"obs_prev\"] = df_hi[\"Cases\"] / df_hi[\"Population\n",
    "\n",
    "df_lo[\"theta_gp\"] = theta_mean_gp[0:n_lo]\n",
    "df_hi[\"theta_gp\"] = theta_mean_gp[n_lo:n_lo + n_hi]\n",
    "\n",
    "theta_obs_lo = df_lo[\"obs_prev\"]\n",
    "theta_gp_est_lo = df_lo[\"theta_gp\"]\n",
    "theta_obs_hi = df_hi[\"obs_prev\"]\n",
    "theta_gp_est_hi = df_hi[\"theta_gp\"]\n",
    "\n",
    "max_val_lo = np.max([theta_obs_lo, theta_gp_est_lo])\n",
    "min_val_lo = np.min([theta_obs_lo, theta_gp_est_lo])\n",
    "\n",
    "max_val_hi = np.max([theta_obs_hi, theta_gp_est_hi])\n",
    "min_val_hi = np.min([theta_obs_hi, theta_gp_est_hi])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "565SkR5Rq72t"
   },
   "source": [
    "# Plot the map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GX-zKBypUerz"
   },
   "source": [
    "##  (low-res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "N_P4BfMmqy92",
    "outputId": "70dbcbbd-008e-4bc4-8454-cc31a6b26eb6"
   },
   "outputs": [],
   "source": [
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot observed prevalence on the map\n",
    "df_lo.plot(\n",
    "    column=\"obs_prev\",  # Column to use for color\n",
    "    cmap=\"viridis\",  # Colormap\n",
    "    vmin=min_val_lo,  # Minimum value for color scale\n",
    "    vmax=max_val_lo,  # Maximum value for color scale\n",
    "    legend=True,  # Show legend\n",
    "    ax=ax[0],  # Plot on the first subplot\n",
    ")\n",
    "ax[0].set_title(\"Low-Res Observed Prevalence\")\n",
    "\n",
    "# Plot estimated prevalence on the map\n",
    "df_lo.plot(\n",
    "    column=\"theta_gp\",  # Column to use for color\n",
    "    cmap=\"viridis\",  # Colormap\n",
    "    vmin=min_val_lo,  # Minimum value for color scale\n",
    "    vmax=max_val_lo,  # Maximum value for color scale\n",
    "    legend=True,  # Show legend\n",
    "    ax=ax[1],  # Plot on the second subplot\n",
    ")\n",
    "ax[1].set_title(\"Low-Res Estimated Prevalence (θ)\")\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"observed_vs_estimated_prevalence_lo.png\")  # Save as PNG\n",
    "# Or save as PDF:\n",
    "# plt.savefig(\"observed_vs_estimated_prevalence_lo.pdf\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BD_7w9ZbUryB"
   },
   "source": [
    "## high-res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZU5isbe8Ut1O"
   },
   "outputs": [],
   "source": [
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot observed prevalence on the map\n",
    "df_hi.plot(\n",
    "    column=\"obs_prev\",  # Column to use for color\n",
    "    cmap=\"viridis\",  # Colormap\n",
    "    vmin=min_val_hi,  # Minimum value for color scale\n",
    "    vmax=max_val_hi,  # Maximum value for color scale\n",
    "    legend=True,  # Show legend\n",
    "    ax=ax[0],  # Plot on the first subplot\n",
    ")\n",
    "ax[0].set_title(\"High-Res Observed Prevalence\")\n",
    "\n",
    "# Plot estimated prevalence on the map\n",
    "df_hi.plot(\n",
    "    column=\"theta_gp\",  # Column to use for color\n",
    "    cmap=\"viridis\",  # Colormap\n",
    "    vmin=min_val_hi,  # Minimum value for color scale\n",
    "    vmax=max_val_hi,  # Maximum value for color scale\n",
    "    legend=True,  # Show legend\n",
    "    ax=ax[1],  # Plot on the second subplot\n",
    ")\n",
    "ax[1].set_title(\"High-Res Estimated Prevalence (θ)\")\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"observed_vs_estimated_prevalence_hi.png\")  # Save as PNG\n",
    "# Or save as PDF:\n",
    "# plt.savefig(\"observed_vs_estimated_prevalence.pdf\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#save the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-aHDb-ko73z"
   },
   "source": [
    "###Plot scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "2hGxQTW2nr6j",
    "outputId": "bc0b05cf-9ddf-487c-98b5-adada74fd181"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert JAX arrays to NumPy arrays before plotting\n",
    "def plot_scatter(ax, df, bci_25, bci_75, title, color):\n",
    "    theta_obs = np.asarray(df[\"obs_prev\"])\n",
    "    theta_gp_est = np.asarray(df[\"theta_gp\"])\n",
    "    bci_25 = np.asarray(bci_25)\n",
    "    bci_75 = np.asarray(bci_75)\n",
    "\n",
    "    # Compute min and max for plot limits\n",
    "    _max = max(theta_obs.max(), theta_gp_est.max())\n",
    "    _min = min(theta_obs.min(), theta_gp_est.min())\n",
    "\n",
    "    # Scatter plot of observed vs estimated prevalence\n",
    "    sns.scatterplot(x=theta_obs, y=theta_gp_est, label=\"Observed vs Estimated\", color=color, ax=ax)\n",
    "\n",
    "    # Add IQR shaded region\n",
    "    ax.fill_between(theta_obs, bci_25, bci_75, color=color, alpha=0.2, label=\"IQR (25%-75%)\")\n",
    "\n",
    "    # Plot reference line (y = x) for comparison\n",
    "    sns.lineplot(x=[_min, _max], y=[_min, _max], color=\"red\", linestyle=\"--\", label=\"y = x\", ax=ax)\n",
    "\n",
    "    ax.set_xlabel(\"Observed Prevalence\")\n",
    "    ax.set_ylabel(\"Estimated Prevalence (θ)\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot for low-res data\n",
    "plot_scatter(axes[0], df_lo, bci_gp_25, bci_gp_75, \"Low-Res Observed vs Estimated Prevalence\", \"blue\")\n",
    "\n",
    "# Plot for high-res data\n",
    "plot_scatter(axes[1], df_hi, bci_hi_25, bci_hi_75, \"High-Res Observed vs Estimated Prevalence\", \"green\")\n",
    "\n",
    "# Adjust layout and save figure\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"observed_vs_estimated_prevalence_scatter_side_by_side.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogViomPZUpkM"
   },
   "outputs": [],
   "source": [
    "#save df\n",
    "df_lo.to_csv(\"../data/df_with_gp_preds_lowres.csv\")\n",
    "df_lo.to_file(\"../data/df_with_gp_preds_lowres.shp\")\n",
    "df_hi.to_file(\"../data/df_with_gp_preds_highres.shp\")\n",
    "df_hi.to_csv(\"../data/df_with_gp_preds_highres.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
