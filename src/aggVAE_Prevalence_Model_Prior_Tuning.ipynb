{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94AFUwsyPklX"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "avbIqnrEPU1E"
      },
      "outputs": [],
      "source": [
        "import arviz as az\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "import geopandas as gpd\n",
        "import dill\n",
        "import numpyro\n",
        "import numpyro.distributions as dist\n",
        "from numpyro.infer import MCMC, NUTS\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import time\n",
        "\n",
        "import itertools\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, lax, jit, ops\n",
        "from jax.example_libraries import stax\n",
        "\n",
        "import numpyro\n",
        "from numpyro.infer import SVI, MCMC, NUTS, init_to_median, Predictive, RenyiELBO, log_likelihood\n",
        "import numpyro.distributions as dist\n",
        "\n",
        "import geopandas as gpd\n",
        "import plotly.express as px\n",
        "\n",
        "from termcolor import colored\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg1XU4rzG9XG"
      },
      "source": [
        "### Load the final_combined_divisions data to get the observed prevalence counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QBA2QbqrHBs_"
      },
      "outputs": [],
      "source": [
        "df = gpd.read_file(\"../data/processed/final_combined_divisions/final_combined_divisions.shp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zf62molpQ4hu"
      },
      "outputs": [],
      "source": [
        "true_values = df[\"Cases\"]/df['Population']  # observed prevalence values\n",
        "total_cases = df['Cases']  # your total cases\n",
        "total_population = df['Population']  # your total population"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8p4jSHmQDRR"
      },
      "source": [
        "### Load the mcmc data from all 4 different sigma priors (1, 0.01, 0.0001, 0.000001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdCnn3pHQC4V"
      },
      "outputs": [],
      "source": [
        "#for sigma 1e-2\n",
        "with open(\"../model_weights/aggVAEPrev/aggVAEPrev_nsamples_2000_tt0min_sigma0.01_hdim50_zdim20.pkl\", \"rb\") as file:\n",
        "        sigma_1 = dill.load(file)\n",
        "\n",
        "#for sigma 1e-1\n",
        "with open(\"../model_weights/aggVAEPrev/aggVAEPrev_nsamples_2000_tt0min_sigma0.1_hdim50_zdim20.pkl\", \"rb\") as file:\n",
        "        sigma_2 = dill.load(file)\n",
        "\n",
        "#for sigma 1\n",
        "with open(\"../model_weights/aggVAEPrev/aggVAEPrev_nsamples_2000_tt1min_sigma1_hdim50_zdim20.pkl\", \"rb\") as file:\n",
        "        sigma_3 = dill.load(file)\n",
        "\n",
        "#for sigma 5\n",
        "with open(\"../model_weights/aggVAEPrev/aggVAEPrev_nsamples_2000_tt2min_sigma5_hdim50_zdim20.pkl\", \"rb\") as file:\n",
        "        sigma_4 = dill.load(file)\n",
        "\n",
        "#for sigma 10\n",
        "with open(\"../model_weights/aggVAEPrev/aggVAEPrev_nsamples_2000_tt2min_sigma10_hdim50_zdim20.pkl\", \"rb\") as file:\n",
        "        sigma_5 = dill.load(file)\n",
        "\n",
        "#for sigma 50\n",
        "with open(\"../model_weights/aggVAEPrev/aggVAEPrev_nsamples_2000_tt2min_sigma50_hdim50_zdim20.pkl\", \"rb\") as file:\n",
        "        sigma_6 = dill.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "mcmc_dict = {\n",
        "    \"sigma 1e-2\": sigma_1,\n",
        "    \"sigma 1e-1\": sigma_2,\n",
        "    \"sigma 1\": sigma_3,\n",
        "    \"sigma 5\": sigma_4,\n",
        "    \"sigma 10\": sigma_5,\n",
        "    \"sigma 50\": sigma_6\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available keys in ss: dict_keys(['b0', 'b_hdi', 'b_pop_density', 'sigma', 'theta', 'vae', 'vae_aggr', 'z'])\n",
            "Average ESS for all aggVAE effects for sigma 1e-2 prior: 5665\n",
            "Max R-hat for all aggVAE effects for sigma 1e-2 prior: 1.0\n",
            "Available keys in ss: dict_keys(['b0', 'b_hdi', 'b_pop_density', 'sigma', 'theta', 'vae', 'vae_aggr', 'z'])\n",
            "Average ESS for all aggVAE effects for sigma 1e-1 prior: 421\n",
            "Max R-hat for all aggVAE effects for sigma 1e-1 prior: 15.539999961853027\n",
            "Available keys in ss: dict_keys(['b0', 'b_hdi', 'b_pop_density', 'sigma', 'theta', 'vae', 'vae_aggr', 'z'])\n",
            "Average ESS for all aggVAE effects for sigma 1 prior: 178\n",
            "Max R-hat for all aggVAE effects for sigma 1 prior: 15.279999732971191\n",
            "Available keys in ss: dict_keys(['b0', 'b_hdi', 'b_pop_density', 'sigma', 'theta', 'vae', 'vae_aggr', 'z'])\n",
            "Average ESS for all aggVAE effects for sigma 5 prior: 46\n",
            "Max R-hat for all aggVAE effects for sigma 5 prior: 33.0099983215332\n",
            "Available keys in ss: dict_keys(['b0', 'b_hdi', 'b_pop_density', 'sigma', 'theta', 'vae', 'vae_aggr', 'z'])\n",
            "Average ESS for all aggVAE effects for sigma 10 prior: 3137\n",
            "Max R-hat for all aggVAE effects for sigma 10 prior: 1.0\n",
            "Available keys in ss: dict_keys(['b0', 'b_hdi', 'b_pop_density', 'sigma', 'theta', 'vae', 'vae_aggr', 'z'])\n",
            "Average ESS for all aggVAE effects for sigma 50 prior: 2\n",
            "Max R-hat for all aggVAE effects for sigma 50 prior: 31.68000030517578\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numpyro.diagnostics import summary\n",
        "\n",
        "for prior_name, mcmc in mcmc_dict.items():\n",
        "    samples = mcmc.get_samples(group_by_chain=True)\n",
        "    ss = summary(samples)\n",
        "\n",
        "    # Debugging: Print available keys\n",
        "    print(\"Available keys in ss:\", ss.keys())\n",
        "\n",
        "    # Compute and print diagnostics\n",
        "    r = np.mean(ss[\"vae_aggr\"][\"n_eff\"])\n",
        "    print(f\"Average ESS for all aggVAE effects for {prior_name} prior: {round(r)}\")\n",
        "    print(f\"Max R-hat for all aggVAE effects for {prior_name} prior: {round(np.max(ss['vae_aggr']['r_hat']), 2)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbGuINGPLhUF",
        "outputId": "68abc52a-a44c-42a7-f2a4-2a9dadd94228"
      },
      "outputs": [],
      "source": [
        "idata_1 = az.from_dict(posterior = sigma_1.get_samples(group_by_chain = True))\n",
        "idata_2 = az.from_dict(posterior = sigma_2.get_samples(group_by_chain = True))\n",
        "idata_3 = az.from_dict(posterior = sigma_3.get_samples(group_by_chain = True))\n",
        "idata_4 = az.from_dict(posterior = sigma_4.get_samples(group_by_chain = True))\n",
        "idata_5 = az.from_dict(posterior = sigma_5.get_samples(group_by_chain = True))\n",
        "idata_6 = az.from_dict(posterior = sigma_6.get_samples(group_by_chain = True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz6ETuMr5Q0y"
      },
      "source": [
        "### Load the model and parameters for aggVAEPrev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XznIM0tB5P1n"
      },
      "outputs": [],
      "source": [
        "#define the necessary functions\n",
        "def dist_euclid(x, z):\n",
        "    \"\"\"\n",
        "    Computes Eucledian Distance Between Regions. This function is used by\n",
        "    exp_sq_kernel function (kernel function for gaussian processes)\n",
        "    \"\"\"\n",
        "    x = jnp.array(x) # (ngrid_pts, lat/lon) <- i.e (7304,2)\n",
        "    z = jnp.array(z) # (ngrid_pts, lat/lon) <- i.e (7304,2)\n",
        "    if len(x.shape)==1:\n",
        "        x = x.reshape(x.shape[0], 1) #(2618,) -> (7304,1)\n",
        "    if len(z.shape)==1:\n",
        "        z = x.reshape(x.shape[0], 1) #(2618,) -> (7304,1)\n",
        "    n_x, m = x.shape # 7304 , 2\n",
        "    n_z, m_z = z.shape # 7304 , 2\n",
        "    assert m == m_z\n",
        "    delta = jnp.zeros((n_x,n_z)) #(ngrid_pts,ngrid_pts) <- i.e (7304,7304)\n",
        "    for d in jnp.arange(m):\n",
        "        x_d = x[:,d] #(ngrid_pts-lat/lon,) <- (7304,)\n",
        "        z_d = z[:,d] #(ngrid_pts-lat/lon,) <- (7304,)\n",
        "        delta += (x_d[:,jnp.newaxis] - z_d)**2 # (7304,7304)\n",
        "\n",
        "    return jnp.sqrt(delta) #(7304,7304)\n",
        "def exp_sq_kernel(x, z, var, length, noise, jitter=1.0e-4):\n",
        "    dist = dist_euclid(x, z) #(7304, 7304)\n",
        "    deltaXsq = jnp.power(dist/ length, 2.0)\n",
        "    k = var * jnp.exp(-0.5 * deltaXsq)\n",
        "    k += (noise + jitter) * jnp.eye(x.shape[0])\n",
        "    return k # (ngrid_pts, ngrid_pts) <- (7304,7304)\n",
        "\n",
        "def vae_decoder(hidden_dim, out_dim):\n",
        "    return stax.serial(\n",
        "        # (num_samples, z_dim) -> (num_samples, hidden_dim): (5,40) -> (5,50)\n",
        "        stax.Dense(hidden_dim, W_init = stax.randn()),\n",
        "        stax.Elu,\n",
        "        # (num_samples, hidden_dim) -> (num_samples, num_regions) : (5,50) -> (5, 58)\n",
        "        stax.Dense(out_dim, W_init = stax.randn())\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BzoAi4rQ5pPR"
      },
      "outputs": [],
      "source": [
        "def prev_model_vae_aggr(args):\n",
        "\n",
        "    x = args[\"x\"]\n",
        "    out_dims = args[\"out_dims\"]\n",
        "    pop_density = args[\"pop_density\"]\n",
        "    hdi = args[\"hdi\"]\n",
        "    total_cases = args[\"total_cases\"]\n",
        "    total_population = args[\"total_population\"]\n",
        "    n = args[\"sigma\"]\n",
        "\n",
        "    # random effect\n",
        "    decoder_params =args[\"decoder_params\"]\n",
        "    z_dim, hidden_dim = decoder_params[0][0].shape\n",
        "    z = numpyro.sample(\"z\", dist.Normal(jnp.zeros(z_dim), jnp.ones(z_dim)))\n",
        "    _, decoder_apply = vae_decoder(hidden_dim, out_dims) #Instantiate decoder\n",
        "    vae_aggr = numpyro.deterministic(\"vae_aggr\", decoder_apply(decoder_params, z))\n",
        "    s = numpyro.sample(\"sigma\", dist.HalfNormal(n))\n",
        "    vae = numpyro.deterministic(\"vae\", s * vae_aggr)\n",
        "\n",
        "    ## Fixed effects\n",
        "    b0 = numpyro.sample(\"b0\", dist.Normal(0, 1))  #Intercept\n",
        "    b_pop_density = numpyro.sample(\"b_pop_density\", dist.Normal(0, 1))  #Effect of population density\n",
        "    b_hdi = numpyro.sample(\"b_hdi\", dist.Normal(0, 1))  #Effect of HDI\n",
        "\n",
        "    #scale pop_density and hdi (normalise)\n",
        "    # Standardize covariates\n",
        "    pop_density = (pop_density - jnp.mean(pop_density)) / (jnp.std(pop_density))\n",
        "    hdi = (hdi - jnp.mean(hdi)) / (jnp.std(hdi))\n",
        "\n",
        "    # Linear predictor\n",
        "    lp = b0 + vae + b_pop_density * pop_density + b_hdi * hdi  # (num_districts,)\n",
        "\n",
        "    # Prevalence probability\n",
        "    theta = numpyro.deterministic(\"theta\", jax.nn.sigmoid(lp)*1e-2)  # (num_districts,)\n",
        "\n",
        "    # Binomial likelihood\n",
        "    observed_cases = numpyro.sample(\n",
        "        \"observed_cases\",\n",
        "        dist.Binomial(total_count=total_population, probs = theta),\n",
        "        obs=total_cases\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dLnLIclt5siv"
      },
      "outputs": [],
      "source": [
        "# Lat/Lon Values of artificial grid\n",
        "x = np.load(\"../data/processed/lat_lon_x_all.npy\")\n",
        "\n",
        "# combined regional data\n",
        "pol_pts_all = np.load(\"../data/processed/pol_pts_all.npy\")\n",
        "pt_which_pol_all = np.load(\"../data/processed/pt_which_pol_all.npy\")\n",
        "\n",
        "#combine the dataframes\n",
        "df_combined = gpd.read_file(\"../data/processed/final_combined_divisions/final_combined_divisions.shp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fBN3AeCC5tEp"
      },
      "outputs": [],
      "source": [
        "M = pol_pts_all\n",
        "out_dims = df_combined.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "USAHh33C5uiW"
      },
      "outputs": [],
      "source": [
        "args = {\n",
        "        \"total_cases\" : jnp.array(df_combined[\"Cases\"]),\n",
        "        \"total_population\" : jnp.array(df_combined[\"Population\"]),\n",
        "        \"hdi\" : jnp.array(df_combined[\"HDI\"]),\n",
        "        \"pop_density\" : jnp.array(df_combined[\"Pop_den\"]),\n",
        "        \"x\" : jnp.array(x),\n",
        "        \"gp_kernel\" : exp_sq_kernel,\n",
        "        \"jitter\" : 1e-4,\n",
        "        \"noise\" : 1e-4,\n",
        "        \"M\" : M,\n",
        "        # VAE training\n",
        "        \"rng_key\": random.PRNGKey(5),\n",
        "        \"num_epochs\": 20,\n",
        "        \"learning_rate\": 0.0005,\n",
        "        \"batch_size\": 100,\n",
        "        \"out_dims\" : out_dims,\n",
        "        \"num_train\": 100,\n",
        "        \"num_test\":100,\n",
        "        \"vae_var\": 1,\n",
        "        #default optimal sigma\n",
        "        \"sigma\": 50\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ShW2OC1i5xDw"
      },
      "outputs": [],
      "source": [
        "#change the specific file name under the folder model_weights\n",
        "with open(\"../model_weights/aggVAE/aggVAE_e20_h50_z20\", \"rb\") as file:\n",
        "        vae_params = pickle.load(file)\n",
        "\n",
        "encoder_params = vae_params[\"encoder$params\"]\n",
        "decoder_params = vae_params[\"decoder$params\"]\n",
        "args[\"decoder_params\"] = decoder_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHYqsBJdVouh"
      },
      "source": [
        "### Append all the theta estimates correctly to df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "A6DI5uI9Vr2y"
      },
      "outputs": [],
      "source": [
        "# Assuming you have posterior samples for different priors\n",
        "# e.g., theta samples for different priors: sigma_1, sigma_1e-2, sigma_1e-3, sigma_1e-5\n",
        "\n",
        "# Extract posterior samples for each prior (as per your example)\n",
        "theta_samps_sigma_1 = sigma_1.get_samples()[\"theta\"]\n",
        "theta_samps_sigma_2 = sigma_2.get_samples()[\"theta\"]\n",
        "theta_samps_sigma_3 = sigma_3.get_samples()[\"theta\"]\n",
        "theta_samps_sigma_4 = sigma_4.get_samples()[\"theta\"]\n",
        "theta_samps_sigma_5 = sigma_5.get_samples()[\"theta\"]\n",
        "theta_samps_sigma_6 = sigma_6.get_samples()[\"theta\"]\n",
        "\n",
        "# Compute mean and quantiles (if needed) for theta\n",
        "theta_mean_sigma_1 = theta_samps_sigma_1.mean(axis=(0, 1))\n",
        "theta_mean_sigma_2 = theta_samps_sigma_2.mean(axis=(0, 1))\n",
        "theta_mean_sigma_3 = theta_samps_sigma_3.mean(axis=(0, 1))\n",
        "theta_mean_sigma_4 = theta_samps_sigma_4.mean(axis=(0, 1))\n",
        "theta_mean_sigma_5 = theta_samps_sigma_5.mean(axis=(0, 1))\n",
        "theta_mean_sigma_6 = theta_samps_sigma_6.mean(axis=(0, 1))\n",
        "\n",
        "# Append the theta estimates for each prior to the DataFrame\n",
        "df[\"theta_vae_aggr_sigma_1\"] = theta_mean_sigma_1\n",
        "df[\"theta_vae_aggr_sigma_2\"] = theta_mean_sigma_2\n",
        "df[\"theta_vae_aggr_sigma_3\"] = theta_mean_sigma_3\n",
        "df[\"theta_vae_aggr_sigma_4\"] = theta_mean_sigma_4\n",
        "df[\"theta_vae_aggr_sigma_5\"] = theta_mean_sigma_5\n",
        "df[\"theta_vae_aggr_sigma_6\"] = theta_mean_sigma_6\n",
        "\n",
        "# Ensure that the observed prevalence (obs_prev) is also included\n",
        "df[\"obs_prev\"] = df[\"Cases\"] / df[\"Population\"]\n",
        "theta_observed = df[\"obs_prev\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYSEpy1b5NJW",
        "outputId": "89bb962f-2f6d-43df-b408-9557a8c4da41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'observed_cases': Array([[ -476.125   ,   -79.72412 , -4713.639   , ..., -1681.7864  ,\n",
            "         -428.10693 , -1769.157   ],\n",
            "       [ -443.68506 ,   -66.55957 , -4574.645   , ..., -1699.165   ,\n",
            "         -438.13135 , -1793.2725  ],\n",
            "       [ -474.81445 ,   -83.197266, -4703.9146  , ..., -1688.1946  ,\n",
            "         -428.28345 , -1776.8489  ],\n",
            "       ...,\n",
            "       [ -474.44678 ,   -79.43701 , -4688.0864  , ..., -1675.5502  ,\n",
            "         -421.87866 , -1762.2217  ],\n",
            "       [ -468.30176 ,   -76.28076 , -4662.1743  , ..., -1684.3828  ,\n",
            "         -428.9812  , -1775.2847  ],\n",
            "       [ -483.4702  ,   -82.07422 , -4729.5093  , ..., -1659.873   ,\n",
            "         -414.37427 , -1751.571   ]], dtype=float32)}\n",
            "{'observed_cases': Array([[  -22.356445,  -449.7627  , -2796.5098  , ...,  -323.72455 ,\n",
            "           -9.378418,  -346.41235 ],\n",
            "       [  -25.254883,  -425.73438 , -2800.287   , ...,  -324.51312 ,\n",
            "           -9.050049,  -341.03162 ],\n",
            "       [  -25.983398,  -417.70508 , -2828.9443  , ...,  -339.67218 ,\n",
            "           -8.680298,  -350.69128 ],\n",
            "       ...,\n",
            "       [  -26.570312,  -272.3955  , -2536.0547  , ...,  -357.44257 ,\n",
            "          -11.055054,  -377.85156 ],\n",
            "       [  -25.44629 ,  -275.93262 , -2539.1943  , ...,  -355.96204 ,\n",
            "          -10.548462,  -376.35474 ],\n",
            "       [  -29.605469,  -258.6416  , -2565.916   , ...,  -351.90625 ,\n",
            "          -11.338257,  -366.54407 ]], dtype=float32)}\n",
            "{'observed_cases': Array([[  -32.433594,  -435.62012 , -2627.871   , ...,   -85.97595 ,\n",
            "          -72.872925,   -97.37024 ],\n",
            "       [  -29.848633,  -407.24805 , -2653.625   , ...,   -96.308075,\n",
            "          -62.003662,   -98.59131 ],\n",
            "       [  -34.79004 ,  -418.60254 , -2704.497   , ...,   -89.24625 ,\n",
            "          -65.866455,   -96.706726],\n",
            "       ...,\n",
            "       [  -71.555664,  -422.82227 , -2314.2803  , ...,  -103.15192 ,\n",
            "          -52.193054,  -151.92682 ],\n",
            "       [  -60.936523,  -448.54395 , -2215.0088  , ...,  -112.942566,\n",
            "          -42.912292,  -166.92847 ],\n",
            "       [  -53.429688,  -445.68262 , -2145.6426  , ...,  -100.13919 ,\n",
            "          -59.97937 ,  -139.37964 ]], dtype=float32)}\n",
            "{'observed_cases': Array([[  -25.855469,  -316.41602 , -2670.5605  , ...,   -95.61377 ,\n",
            "          -86.86017 ,   -68.7746  ],\n",
            "       [  -25.66504 ,  -326.7129  , -2718.1035  , ...,   -96.79678 ,\n",
            "          -89.65875 ,   -64.855286],\n",
            "       [  -29.658203,  -324.73145 , -2788.87    , ...,   -94.5329  ,\n",
            "          -87.14111 ,   -67.19208 ],\n",
            "       ...,\n",
            "       [  -35.345703,   -93.509766, -1374.1455  , ...,  -294.9762  ,\n",
            "          -93.4494  ,  -222.4035  ],\n",
            "       [  -32.121094,   -93.15527 , -1417.8135  , ...,  -300.82568 ,\n",
            "          -95.095276,  -220.22229 ],\n",
            "       [  -27.046875,   -88.609375, -1415.7822  , ...,  -298.3731  ,\n",
            "          -92.932556,  -231.47693 ]], dtype=float32)}\n",
            "{'observed_cases': Array([[  -31.700195,   -97.56152 , -1356.2275  , ...,  -311.34387 ,\n",
            "          -91.70154 ,  -242.25537 ],\n",
            "       [  -31.53418 ,   -97.59277 , -1434.6807  , ...,  -314.57837 ,\n",
            "         -100.121704,  -227.85883 ],\n",
            "       [  -33.418945,   -98.5918  , -1419.3848  , ...,  -301.2611  ,\n",
            "         -103.123535,  -223.54395 ],\n",
            "       ...,\n",
            "       [  -37.89258 ,  -109.45996 , -1353.081   , ...,  -306.02704 ,\n",
            "          -94.17639 ,  -229.4621  ],\n",
            "       [  -41.3125  ,  -103.56543 , -1326.0576  , ...,  -308.68726 ,\n",
            "          -94.22278 ,  -225.1275  ],\n",
            "       [  -27.580078,   -91.28418 , -1380.0537  , ...,  -315.86847 ,\n",
            "          -89.64813 ,  -251.2799  ]], dtype=float32)}\n",
            "{'observed_cases': Array([[  -29.871094,   -95.27539 , -1376.6953  , ...,  -304.135   ,\n",
            "         -102.71149 ,  -230.2785  ],\n",
            "       [  -31.544922,   -98.399414, -1428.7217  , ...,  -312.5724  ,\n",
            "         -100.65143 ,  -223.99512 ],\n",
            "       [  -31.458008,   -94.12695 , -1437.041   , ...,  -312.4568  ,\n",
            "          -96.0022  ,  -231.84369 ],\n",
            "       ...,\n",
            "       [  -35.14746 ,  -105.84082 , -1363.6992  , ...,  -304.29602 ,\n",
            "          -96.66589 ,  -229.17188 ],\n",
            "       [  -40.231445,  -106.29785 , -1335.5547  , ...,  -312.69397 ,\n",
            "          -96.41266 ,  -229.29865 ],\n",
            "       [  -23.82129 ,   -83.30762 , -1426.4775  , ...,  -312.04407 ,\n",
            "          -96.262634,  -248.72919 ]], dtype=float32)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\jessi\\AppData\\Roaming\\Python\\Python313\\site-packages\\arviz\\data\\base.py:272: UserWarning: More chains (8000) than draws (96). Passed array should have shape (chains, draws, *shape)\n",
            "  warnings.warn(\n",
            "C:\\Users\\jessi\\AppData\\Roaming\\Python\\Python313\\site-packages\\arviz\\data\\base.py:272: UserWarning: More chains (8000) than draws (20). Passed array should have shape (chains, draws, *shape)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Extract log likelihood for each prior\n",
        "# Assume observed_data is the actual data you used for inference\n",
        "# Update args with the observed data for log_likelihood calculation\n",
        "\n",
        "args_with_obs_1 = args.copy()\n",
        "args_with_obs_1[\"observed_cases\"] = np.asarray(true_values) # Assuming 'true_values' is the observed data\n",
        "args_with_obs_1[\"sigma\"] = 0.01\n",
        "log_likelihood_1 = log_likelihood(prev_model_vae_aggr, sigma_1.get_samples(), args_with_obs_1)\n",
        "print(log_likelihood_1)\n",
        "\n",
        "\n",
        "args_with_obs_2 = args.copy()\n",
        "args_with_obs_2[\"observed_cases\"] = np.asarray(true_values) # Assuming 'true_values' is the observed data\n",
        "args_with_obs_2[\"sigma\"] = 0.1\n",
        "log_likelihood_2 = log_likelihood(prev_model_vae_aggr, sigma_2.get_samples(), args_with_obs_2)\n",
        "print(log_likelihood_2)\n",
        "\n",
        "\n",
        "args_with_obs_3 = args.copy()\n",
        "args_with_obs_3[\"observed_cases\"] = np.asarray(true_values) # Assuming 'true_values' is the observed data\n",
        "args_with_obs_3[\"sigma\"] = 1\n",
        "log_likelihood_3 = log_likelihood(prev_model_vae_aggr, sigma_3.get_samples(), args_with_obs_3)\n",
        "print(log_likelihood_3)\n",
        "\n",
        "\n",
        "args_with_obs_4 = args.copy()\n",
        "args_with_obs_4[\"observed_cases\"] = np.asarray(true_values) # Assuming 'true_values' is the observed data\n",
        "args_with_obs_4[\"sigma\"] = 5\n",
        "log_likelihood_4 = log_likelihood(prev_model_vae_aggr, sigma_4.get_samples(), args_with_obs_4)\n",
        "print(log_likelihood_4)\n",
        "\n",
        "\n",
        "args_with_obs_5 = args.copy()\n",
        "args_with_obs_5[\"observed_cases\"] = np.asarray(true_values) # Assuming 'true_values' is the observed data\n",
        "args_with_obs_5[\"sigma\"] = 10\n",
        "log_likelihood_5 = log_likelihood(prev_model_vae_aggr, sigma_5.get_samples(), args_with_obs_5)\n",
        "print(log_likelihood_5)\n",
        "\n",
        "\n",
        "args_with_obs_6 = args.copy()\n",
        "args_with_obs_6[\"observed_cases\"] = np.asarray(true_values) # Assuming 'true_values' is the observed data\n",
        "args_with_obs_6[\"sigma\"] = 50\n",
        "log_likelihood_6 = log_likelihood(prev_model_vae_aggr, sigma_6.get_samples(), args_with_obs_6)\n",
        "print(log_likelihood_6)\n",
        "\n",
        "#update the idata object with log-likelihood\n",
        "idata_1 = az.from_dict(posterior=sigma_1.get_samples(), log_likelihood={\"obs\": log_likelihood_1})\n",
        "idata_2 = az.from_dict(posterior=sigma_2.get_samples(), log_likelihood={\"obs\": log_likelihood_2})\n",
        "idata_3 = az.from_dict(posterior=sigma_3.get_samples(), log_likelihood={\"obs\": log_likelihood_3})\n",
        "idata_4 = az.from_dict(posterior=sigma_4.get_samples(), log_likelihood={\"obs\": log_likelihood_4})\n",
        "idata_5 = az.from_dict(posterior=sigma_5.get_samples(), log_likelihood={\"obs\": log_likelihood_5})\n",
        "idata_6 = az.from_dict(posterior=sigma_6.get_samples(), log_likelihood={\"obs\": log_likelihood_6})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zgZaDW5o7M25"
      },
      "outputs": [],
      "source": [
        "idata_dict = {\n",
        "    \"Sigma 1e-2\": idata_1,\n",
        "    \"Sigma 1e-1\": idata_2,\n",
        "    \"Sigma 1\": idata_3,\n",
        "    \"Sigma 5\": idata_4,\n",
        "    \"Sigma 10\": idata_5,\n",
        "    \"Sigma 50\": idata_6,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ZRD3yIpHaj8H",
        "outputId": "0e66db2c-ec9c-47d5-ac25-a50b62ede215"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>District</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>Year</th>\n",
              "      <th>Area_sq_km</th>\n",
              "      <th>HDI</th>\n",
              "      <th>Cases</th>\n",
              "      <th>Population</th>\n",
              "      <th>Pop_den</th>\n",
              "      <th>geometry</th>\n",
              "      <th>theta_vae_aggr_sigma_1</th>\n",
              "      <th>theta_vae_aggr_sigma_2</th>\n",
              "      <th>theta_vae_aggr_sigma_3</th>\n",
              "      <th>theta_vae_aggr_sigma_4</th>\n",
              "      <th>theta_vae_aggr_sigma_5</th>\n",
              "      <th>theta_vae_aggr_sigma_6</th>\n",
              "      <th>obs_prev</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BANDUNG</td>\n",
              "      <td>107.610841</td>\n",
              "      <td>-7.099969</td>\n",
              "      <td>2020</td>\n",
              "      <td>1767.96</td>\n",
              "      <td>72.39</td>\n",
              "      <td>9180</td>\n",
              "      <td>14495160</td>\n",
              "      <td>8198.805403</td>\n",
              "      <td>POLYGON ((107.73309 -6.814, 107.73354 -6.81427...</td>\n",
              "      <td>0.0008217927</td>\n",
              "      <td>0.0009509846</td>\n",
              "      <td>0.00092862523</td>\n",
              "      <td>0.00093148515</td>\n",
              "      <td>0.0009252073</td>\n",
              "      <td>0.0009305632</td>\n",
              "      <td>0.000633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BANDUNG</td>\n",
              "      <td>107.610841</td>\n",
              "      <td>-7.099969</td>\n",
              "      <td>2021</td>\n",
              "      <td>1767.96</td>\n",
              "      <td>72.73</td>\n",
              "      <td>8008</td>\n",
              "      <td>14662620</td>\n",
              "      <td>8293.524740</td>\n",
              "      <td>POLYGON ((107.73309 -6.814, 107.73354 -6.81427...</td>\n",
              "      <td>0.0008217927</td>\n",
              "      <td>0.0009509846</td>\n",
              "      <td>0.00092862523</td>\n",
              "      <td>0.00093148515</td>\n",
              "      <td>0.0009252073</td>\n",
              "      <td>0.0009305632</td>\n",
              "      <td>0.000546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BANDUNG</td>\n",
              "      <td>107.610841</td>\n",
              "      <td>-7.099969</td>\n",
              "      <td>2022</td>\n",
              "      <td>1767.96</td>\n",
              "      <td>73.16</td>\n",
              "      <td>16764</td>\n",
              "      <td>14830092</td>\n",
              "      <td>8388.250865</td>\n",
              "      <td>POLYGON ((107.73309 -6.814, 107.73354 -6.81427...</td>\n",
              "      <td>0.0008217927</td>\n",
              "      <td>0.0009509846</td>\n",
              "      <td>0.00092862523</td>\n",
              "      <td>0.00093148515</td>\n",
              "      <td>0.0009252073</td>\n",
              "      <td>0.0009305632</td>\n",
              "      <td>0.001130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BANDUNG</td>\n",
              "      <td>107.610841</td>\n",
              "      <td>-7.099969</td>\n",
              "      <td>2023</td>\n",
              "      <td>1767.96</td>\n",
              "      <td>73.74</td>\n",
              "      <td>4020</td>\n",
              "      <td>14997564</td>\n",
              "      <td>8482.976990</td>\n",
              "      <td>POLYGON ((107.73309 -6.814, 107.73354 -6.81427...</td>\n",
              "      <td>0.0008217927</td>\n",
              "      <td>0.0009509846</td>\n",
              "      <td>0.00092862523</td>\n",
              "      <td>0.00093148515</td>\n",
              "      <td>0.0009252073</td>\n",
              "      <td>0.0009305632</td>\n",
              "      <td>0.000268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BANDUNG BARAT</td>\n",
              "      <td>107.414953</td>\n",
              "      <td>-6.897056</td>\n",
              "      <td>2020</td>\n",
              "      <td>1305.77</td>\n",
              "      <td>68.08</td>\n",
              "      <td>3864</td>\n",
              "      <td>7153344</td>\n",
              "      <td>5478.257273</td>\n",
              "      <td>POLYGON ((107.40945 -6.68851, 107.40986 -6.688...</td>\n",
              "      <td>0.0008217927</td>\n",
              "      <td>0.0009509846</td>\n",
              "      <td>0.00092862523</td>\n",
              "      <td>0.00093148515</td>\n",
              "      <td>0.0009252073</td>\n",
              "      <td>0.0009305632</td>\n",
              "      <td>0.000540</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        District           x         y  Year  Area_sq_km    HDI  Cases  \\\n",
              "0        BANDUNG  107.610841 -7.099969  2020     1767.96  72.39   9180   \n",
              "1        BANDUNG  107.610841 -7.099969  2021     1767.96  72.73   8008   \n",
              "2        BANDUNG  107.610841 -7.099969  2022     1767.96  73.16  16764   \n",
              "3        BANDUNG  107.610841 -7.099969  2023     1767.96  73.74   4020   \n",
              "4  BANDUNG BARAT  107.414953 -6.897056  2020     1305.77  68.08   3864   \n",
              "\n",
              "   Population      Pop_den                                           geometry  \\\n",
              "0    14495160  8198.805403  POLYGON ((107.73309 -6.814, 107.73354 -6.81427...   \n",
              "1    14662620  8293.524740  POLYGON ((107.73309 -6.814, 107.73354 -6.81427...   \n",
              "2    14830092  8388.250865  POLYGON ((107.73309 -6.814, 107.73354 -6.81427...   \n",
              "3    14997564  8482.976990  POLYGON ((107.73309 -6.814, 107.73354 -6.81427...   \n",
              "4     7153344  5478.257273  POLYGON ((107.40945 -6.68851, 107.40986 -6.688...   \n",
              "\n",
              "  theta_vae_aggr_sigma_1 theta_vae_aggr_sigma_2 theta_vae_aggr_sigma_3  \\\n",
              "0           0.0008217927           0.0009509846          0.00092862523   \n",
              "1           0.0008217927           0.0009509846          0.00092862523   \n",
              "2           0.0008217927           0.0009509846          0.00092862523   \n",
              "3           0.0008217927           0.0009509846          0.00092862523   \n",
              "4           0.0008217927           0.0009509846          0.00092862523   \n",
              "\n",
              "  theta_vae_aggr_sigma_4 theta_vae_aggr_sigma_5 theta_vae_aggr_sigma_6  \\\n",
              "0          0.00093148515           0.0009252073           0.0009305632   \n",
              "1          0.00093148515           0.0009252073           0.0009305632   \n",
              "2          0.00093148515           0.0009252073           0.0009305632   \n",
              "3          0.00093148515           0.0009252073           0.0009305632   \n",
              "4          0.00093148515           0.0009252073           0.0009305632   \n",
              "\n",
              "   obs_prev  \n",
              "0  0.000633  \n",
              "1  0.000546  \n",
              "2  0.001130  \n",
              "3  0.000268  \n",
              "4  0.000540  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLecQOfOPnkY"
      },
      "source": [
        "### Define Functions to plot different posterior data using different priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "KYdIcrBDOZPe"
      },
      "outputs": [],
      "source": [
        "def compute_model_comparison(idata_dict):\n",
        "    loo_scores = {}\n",
        "    waic_scores = {}\n",
        "    rhat_values = {}\n",
        "    avg_rhat_values = {}\n",
        "\n",
        "    for prior_name, idata in idata_dict.items():\n",
        "        # Compute LOO using stored log-likelihood\n",
        "        loo_result = az.loo(idata, pointwise=True)\n",
        "        loo_scores[prior_name] = (\n",
        "            loo_result[\"elpd_loo\"].item(),\n",
        "            loo_result[\"se\"].item()\n",
        "        )\n",
        "\n",
        "        # Compute WAIC\n",
        "        waic_result = az.waic(idata, pointwise=True)\n",
        "        waic_scores[prior_name] = (\n",
        "            waic_result[\"elpd_waic\"].item(),\n",
        "            waic_result[\"se\"].item()\n",
        "        )\n",
        "\n",
        "        # Compute R-hat\n",
        "        rhat = az.rhat(idata)\n",
        "        rhat_values[prior_name] = rhat\n",
        "        avg_rhat_values[prior_name] = np.mean(rhat.to_array().values)\n",
        "\n",
        "        print(f\"{prior_name} - LOO: {loo_scores[prior_name][0]:.2f} ± {loo_scores[prior_name][1]:.2f}\")\n",
        "        print(f\"{prior_name} - WAIC: {waic_scores[prior_name][0]:.2f} ± {waic_scores[prior_name][1]:.2f}\")\n",
        "        print(f\"{prior_name} - Average R-hat: {avg_rhat_values[prior_name]:.3f}\")\n",
        "\n",
        "    best_model = min(avg_rhat_values, key=avg_rhat_values.get)\n",
        "    print(f\"\\nBest model based on R-hat: {best_model} with average R-hat of {avg_rhat_values[best_model]:.3f}\")\n",
        "\n",
        "    return {\n",
        "        \"LOO\": loo_scores,\n",
        "        \"WAIC\": waic_scores,\n",
        "        \"R-hat\": rhat_values,\n",
        "        \"Avg R-hat\": avg_rhat_values,\n",
        "        \"Best Model\": best_model\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyGHtA2oOkTk"
      },
      "source": [
        "### Compute the metrics for different samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MssEvnSoRTk1",
        "outputId": "3adbdf0b-6d5a-434c-fa5b-2d13be72ebef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Array contains NaN-value.\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1040: RuntimeWarning: overflow encountered in divide\n",
            "  b_ary /= prior_bs * ary[int(n / 4 + 0.5) - 1]\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1043: RuntimeWarning: invalid value encountered in log1p\n",
            "  k_ary = np.log1p(-b_ary[:, None] * ary).mean(axis=1)  # pylint: disable=no-member\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1060: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  sigma = -k_post / b_post\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1655: UserWarning: For one or more samples the posterior variance of the log predictive densities exceeds 0.4. This could be indication of WAIC starting to fail. \n",
            "See http://arxiv.org/abs/1507.04544 for details\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1683: UserWarning: The point-wise WAIC is the same with the sum WAIC, please double check\n",
            "            the Observed RV in your model to make sure it returns element-wise logp.\n",
            "            \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sigma 1e-2 - LOO: nan ± nan\n",
            "Sigma 1e-2 - WAIC: -562532.33 ± 0.00\n",
            "Sigma 1e-2 - Average R-hat: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1040: RuntimeWarning: overflow encountered in divide\n",
            "  b_ary /= prior_bs * ary[int(n / 4 + 0.5) - 1]\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1043: RuntimeWarning: invalid value encountered in log1p\n",
            "  k_ary = np.log1p(-b_ary[:, None] * ary).mean(axis=1)  # pylint: disable=no-member\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1060: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  sigma = -k_post / b_post\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1655: UserWarning: For one or more samples the posterior variance of the log predictive densities exceeds 0.4. This could be indication of WAIC starting to fail. \n",
            "See http://arxiv.org/abs/1507.04544 for details\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1683: UserWarning: The point-wise WAIC is the same with the sum WAIC, please double check\n",
            "            the Observed RV in your model to make sure it returns element-wise logp.\n",
            "            \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sigma 1e-1 - LOO: nan ± nan\n",
            "Sigma 1e-1 - WAIC: -239658.13 ± 0.00\n",
            "Sigma 1e-1 - Average R-hat: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1040: RuntimeWarning: overflow encountered in divide\n",
            "  b_ary /= prior_bs * ary[int(n / 4 + 0.5) - 1]\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1043: RuntimeWarning: invalid value encountered in log1p\n",
            "  k_ary = np.log1p(-b_ary[:, None] * ary).mean(axis=1)  # pylint: disable=no-member\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1060: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  sigma = -k_post / b_post\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1655: UserWarning: For one or more samples the posterior variance of the log predictive densities exceeds 0.4. This could be indication of WAIC starting to fail. \n",
            "See http://arxiv.org/abs/1507.04544 for details\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1683: UserWarning: The point-wise WAIC is the same with the sum WAIC, please double check\n",
            "            the Observed RV in your model to make sure it returns element-wise logp.\n",
            "            \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sigma 1 - LOO: nan ± nan\n",
            "Sigma 1 - WAIC: -165749.63 ± 0.00\n",
            "Sigma 1 - Average R-hat: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1043: RuntimeWarning: invalid value encountered in log1p\n",
            "  k_ary = np.log1p(-b_ary[:, None] * ary).mean(axis=1)  # pylint: disable=no-member\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1060: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  sigma = -k_post / b_post\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1655: UserWarning: For one or more samples the posterior variance of the log predictive densities exceeds 0.4. This could be indication of WAIC starting to fail. \n",
            "See http://arxiv.org/abs/1507.04544 for details\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1683: UserWarning: The point-wise WAIC is the same with the sum WAIC, please double check\n",
            "            the Observed RV in your model to make sure it returns element-wise logp.\n",
            "            \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sigma 5 - LOO: nan ± nan\n",
            "Sigma 5 - WAIC: -122643.59 ± 0.00\n",
            "Sigma 5 - Average R-hat: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1040: RuntimeWarning: overflow encountered in divide\n",
            "  b_ary /= prior_bs * ary[int(n / 4 + 0.5) - 1]\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1043: RuntimeWarning: invalid value encountered in log1p\n",
            "  k_ary = np.log1p(-b_ary[:, None] * ary).mean(axis=1)  # pylint: disable=no-member\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1060: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  sigma = -k_post / b_post\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1655: UserWarning: For one or more samples the posterior variance of the log predictive densities exceeds 0.4. This could be indication of WAIC starting to fail. \n",
            "See http://arxiv.org/abs/1507.04544 for details\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1683: UserWarning: The point-wise WAIC is the same with the sum WAIC, please double check\n",
            "            the Observed RV in your model to make sure it returns element-wise logp.\n",
            "            \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sigma 10 - LOO: nan ± nan\n",
            "Sigma 10 - WAIC: -104673.51 ± 0.00\n",
            "Sigma 10 - Average R-hat: nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1043: RuntimeWarning: invalid value encountered in log1p\n",
            "  k_ary = np.log1p(-b_ary[:, None] * ary).mean(axis=1)  # pylint: disable=no-member\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1060: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  sigma = -k_post / b_post\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1655: UserWarning: For one or more samples the posterior variance of the log predictive densities exceeds 0.4. This could be indication of WAIC starting to fail. \n",
            "See http://arxiv.org/abs/1507.04544 for details\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/arviz/stats/stats.py:1683: UserWarning: The point-wise WAIC is the same with the sum WAIC, please double check\n",
            "            the Observed RV in your model to make sure it returns element-wise logp.\n",
            "            \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sigma 50 - LOO: nan ± nan\n",
            "Sigma 50 - WAIC: -122266.17 ± 0.00\n",
            "Sigma 50 - Average R-hat: nan\n",
            "\n",
            "Best model based on R-hat: Sigma 1e-2 with average R-hat of nan\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Compute Model Comparison Metrics (LOO, WAIC, R-hat, Log-Likelihood)\n",
        "model_comparison_results = compute_model_comparison(idata_dict)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
