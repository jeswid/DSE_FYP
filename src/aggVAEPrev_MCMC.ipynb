{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Load Libraries"
      ],
      "metadata": {
        "id": "axhzObpWloIe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL3YFrEAlWH6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "\n",
        "import numpyro\n",
        "import numpyro.distributions as dist\n",
        "from numpyro.infer import MCMC, NUTS, Predictive\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "sys.path.append(os.path.pardir)\n",
        "from aggGP import exp_sq_kernel\n",
        "from aggVAE import vae_decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define necessary functions"
      ],
      "metadata": {
        "id": "D6ZheH65nr_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exp_sq_kernel(x, z, var, length, noise, jitter=1.0e-4):\n",
        "    dist = dist_euclid(x, z) #(7304, 7304)\n",
        "    deltaXsq = jnp.power(dist/ length, 2.0)\n",
        "    k = var * jnp.exp(-0.5 * deltaXsq)\n",
        "    k += (noise + jitter) * jnp.eye(x.shape[0])\n",
        "    return k # (ngrid_pts, ngrid_pts) <- (7304,7304)"
      ],
      "metadata": {
        "id": "Ateo3Fzjnt0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vae_decoder(hidden_dim, out_dim):\n",
        "    return stax.serial(\n",
        "        # (num_samples, z_dim) -> (num_samples, hidden_dim): (5,40) -> (5,50)\n",
        "        stax.Dense(hidden_dim, W_init = stax.randn()),\n",
        "        stax.Elu,\n",
        "        # (num_samples, hidden_dim) -> (num_samples, num_regions) : (5,50) -> (5, 58)\n",
        "        stax.Dense(out_dim, W_init = stax.randn())\n",
        "    )"
      ],
      "metadata": {
        "id": "kH6yD3Z1nvS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prevalence Disease Modelling"
      ],
      "metadata": {
        "id": "6UiQlCbylsG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prev_model_vae_aggr(args):\n",
        "    \"\"\"Dengue prevalence model with a Variational Autoencoder (VAE)\"\"\"\n",
        "\n",
        "    x = args[\"x\"]  # Spatial grid points: (num_grid_points, 2)\n",
        "    pop_density = args[\"pop_density\"]  # (num_districts,)\n",
        "    hdi = args[\"hdi\"]  # (num_districts,)\n",
        "    M = args[\"M\"]  # (num_districts, num_grid_points) aggregation matrix\n",
        "    total_cases = args[\"total_cases\"]\n",
        "    total_population = args[\"total_population\"]\n",
        "    decoder_params = args[\"decoder_params\"]\n",
        "    out_dims = args[\"out_dims\"]  # (num_districts,)\n",
        "    predict = args[\"predict\"]\n",
        "\n",
        "    # VAE latent variable\n",
        "    z_dim, h_dim = decoder_params[0][0].shape  # (latent_dim, hidden_dim)\n",
        "    z = numpyro.sample(\"z\", dist.Normal(jnp.zeros(z_dim), jnp.ones(z_dim)))  # (latent_dim,)\n",
        "    dec_init_fn, dec_apply_fn = vae_decoder(h_dim, out_dims)  # Instantiate decoder\n",
        "    vae_output = numpyro.deterministic(\"vae_output\", dec_apply_fn(decoder_params, z))  # (num_grid_points,)\n",
        "\n",
        "    # Aggregate VAE values to district level\n",
        "    vae_aggr = numpyro.deterministic(\"vae_aggr\", M @ vae_output)  # (num_districts,)\n",
        "\n",
        "    # Fixed effects\n",
        "    b0 = numpyro.sample(\"b0\", dist.Normal(0, 1))  # Intercept\n",
        "    b_pop_density = numpyro.sample(\"b_pop_density\", dist.Normal(0, 1))  # Effect of population density\n",
        "    b_hdi = numpyro.sample(\"b_hdi\", dist.Normal(0, 1))  # Effect of HDI\n",
        "\n",
        "    # Linear predictor\n",
        "    lp = b0 + vae_aggr + b_pop_density * pop_density + b_hdi * hdi  # (num_districts,)\n",
        "    theta = numpyro.deterministic(\"theta\", jax.nn.sigmoid(lp))  # (num_districts,)\n",
        "\n",
        "    # Binomial likelihood\n",
        "    if not predict:\n",
        "        observed_cases = numpyro.sample(\n",
        "            \"observed_cases\",\n",
        "            dist.Binomial(total_count=total_population, probs=theta),\n",
        "            obs=total_cases\n",
        "        )\n",
        "    else:\n",
        "        observed_cases = numpyro.sample(\n",
        "            \"observed_cases\",\n",
        "            dist.Binomial(total_count=total_population, probs=theta)\n",
        "        )\n",
        "\n",
        "    return observed_cases\n"
      ],
      "metadata": {
        "id": "0U4XIWddluZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load variables"
      ],
      "metadata": {
        "id": "iW_k_IgWnjbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lat/Lon Values of artificial grid\n",
        "x = np.load(\"../data/processed/lat_lon_x_all.npy\")\n",
        "\n",
        "# combined regional data\n",
        "pol_pts_all = np.load(\"../data/processed/pol_pts_all.npy\")\n",
        "pt_which_pol_all = np.load(\"../data/processed/pt_which_pol_all.npy\")\n",
        "\n",
        "#combine the dataframes\n",
        "df_combined = gpd.read_file(\"../data/processed/final_combined_divisions/final_combined_divisions.shp\")"
      ],
      "metadata": {
        "id": "Bb2vEX8Xnk00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load decoder model"
      ],
      "metadata": {
        "id": "m0Iy2LSxn-I4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"../model_weights/aggVAE_Dec_e20_h53_z48\", \"rb\") as file:\n",
        "    vae_params = pickle.load(file)\n",
        "\n",
        "encoder_params = vae_params[\"encoder$params\"]\n",
        "decoder_params = vae_params[\"decoder$params\"]"
      ],
      "metadata": {
        "id": "aHm-BBeeoA9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    \"x\": jnp.array(x),  # Spatial grid points\n",
        "    \"pop_density\": jnp.array(df.Pop_density),  # Population density per district\n",
        "    \"hdi\": jnp.array(df.HDI),  # HDI per district\n",
        "    \"M\": jnp.array(pol_pts_all),  # Aggregation matrix for district-level prevalence\n",
        "    \"total_cases\": jnp.array(df.Cases),  # Observed dengue cases per district\n",
        "    \"total_population\": jnp.array(df.Population),  # Population tested per district\n",
        "\n",
        "    # VAE training\n",
        "    \"rng_key\": random.PRNGKey(5),\n",
        "    \"num_epochs\": 20,\n",
        "    \"learning_rate\": 0.0005,\n",
        "    \"batch_size\": 100,\n",
        "    \"hidden_dim\": 6,\n",
        "    \"z_dim\": 3,\n",
        "    \"num_train\": 100,\n",
        "    \"num_test\":100,\n",
        "    \"vae_var\": 1,\n",
        "\n",
        "    # NN Weights\n",
        "    \"decoder_params\" : decoder_params,\n",
        "    \"out_dims\" : df.shape[0]\n",
        "\n",
        "    # To handle Predictions since np.nans giving us issues\n",
        "    \"predict\" : False,\n",
        "\n",
        "    # Set to True only if you want to see VAE Aggr GP results before running MCMC\n",
        "    \"check_vae_samples\" : True\n",
        "}"
      ],
      "metadata": {
        "id": "cBqEN1BFoBxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MCMC training"
      ],
      "metadata": {
        "id": "PLsOJNQXotHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ Random keys\n",
        "run_key, predict_key = random.split(random.PRNGKey(3))\n",
        "\n",
        "# ðŸ”¹ MCMC settings\n",
        "n_warm = 1000\n",
        "n_samples = 2000\n",
        "n_chains = 4"
      ],
      "metadata": {
        "id": "Vn6efFdNkZii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get script location and define correct save directory (sibling to src/)\n",
        "script_dir = os.getcwd()  # Get current working directory\n",
        "save_dir = os.path.abspath(os.path.join(script_dir, \"..\", \"model_weights\"))  # Move up and into model_weights\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Run MCMC for each chain separately to prevent total loss on crash\n",
        "chain_keys = random.split(run_key, n_chains)  # Precompute keys\n",
        "\n",
        "for chain_id in range(n_chains):\n",
        "    print(f\"\\nRunning Chain {chain_id + 1}/{n_chains}...\")\n",
        "\n",
        "    # Generate a separate key for each chain\n",
        "    chain_run_key = chain_keys[chain_id]\n",
        "\n",
        "    # Initialize MCMC with controlled step size\n",
        "    mcmc = MCMC(NUTS(prev_model_vae_aggr),\n",
        "        num_warmup=n_warm,\n",
        "        num_samples=n_samples,\n",
        "        num_chains=1)\n",
        "\n",
        "    # Run the chain\n",
        "    start = time.time()\n",
        "    mcmc.run(chain_run_key, args)  # Ensure args is a tuple (args,)\n",
        "    end = time.time()\n",
        "    t_elapsed_min = round((end - start) / 60)\n",
        "\n",
        "    # ðŸ”¹ Save after each chain completes\n",
        "    f_path = os.path.join(save_dir, f\"aggVAEPrev_chain{chain_id}_nsamples_{n_samples}_tt{t_elapsed_min}min_logit.pkl\")\n",
        "    with open(f_path, \"wb\") as file:\n",
        "        dill.dump(mcmc, file)\n",
        "\n",
        "    print(f\"Saved Chain {chain_id + 1} to {f_path}\")\n",
        "    print(f\"Time taken: {t_elapsed_min} min\\n\")"
      ],
      "metadata": {
        "id": "1a-4MKTBouYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ Print total elapsed time\n",
        "total_end = time.time()\n",
        "print(\"\\nMCMC Total elapsed time:\", round(total_end), \"s\")\n",
        "print(\"MCMC Total elapsed time:\", round(total_end / 60), \"min\")\n",
        "print(\"MCMC Total elapsed time:\", round(total_end / (60 * 60)), \"h\")"
      ],
      "metadata": {
        "id": "C-0jps7ak8FG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}